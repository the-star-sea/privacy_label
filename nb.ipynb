{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from summarizer.sbert import SBertSummarizer\n",
    "import os\n",
    "import re\n",
    "\n",
    "def remove_non_english(text):\n",
    "    # Regular expression pattern to match non-English characters\n",
    "    pattern = r'[^\\x00-\\x7F]+'\n",
    "    \n",
    "    # Remove non-English characters from the string\n",
    "    clean_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return clean_text\n",
    "#traverse all the txt files in the directory\n",
    "dir_path=\"tool\"\n",
    "#create if not exist\n",
    "output_path=\"tool/process\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "\n",
    "for file in os.listdir(dir_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        body = open(os.path.join(dir_path,file), 'r').read()\n",
    "        body=remove_non_english(body)\n",
    "        \n",
    "        model = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
    "        result = model(body, num_sentences=80)\n",
    "\n",
    "        output_file=open(os.path.join(output_path,file), 'w+')\n",
    "        output_file.write(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output error\n",
      "output error\n",
      "output error\n",
      "output error\n",
      "output error\n",
      "output error\n",
      "output error\n",
      "output error\n",
      "26\n",
      "['com.dji.overSeaStore.txt', 'com.infoscout.receipthog.txt', 'app.usacoupons.txt', 'com.readyrefresh.txt', 'com.walmart.android.txt', 'com.petsmart.consumermobile.txt', 'com.webmap.txt', 'com.safeway.client.android.acme.txt', 'com.ralphlauren.us.app.txt', 'de.stocard.stocard.txt', 'com.safeway.client.android.albertsons.txt', 'com.shopkick.app.txt', 'com.shoptrack.android.txt', 'com.spencermobile.app.txt', 'com.acehardware.rewards.txt', 'com.payrange.payrange.txt', 'com.jpma.EBTShopper.txt', 'com.ebay.mobile.txt', 'com.macys.android.txt', 'com.aftership.AfterShip.txt', 'com.dollargeneral.android.txt', 'com.zennioptical.app.txt', 'com.wegmans.wegmansapp.txt', 'com.app.offerit.txt', 'com.meijer.mobile.meijer.txt', 'fr.vestiairecollective.txt', 'com.airgoat.goat.txt', 'com.woot.android.main.txt', 'com.consumerapp_v1.txt', 'com.shopify.arrive.txt', 'com.mercadolibre.txt', 'de.apptiv.business.android.aldi_us.txt', 'com.coinout.scan.txt', 'com.loves.finder.txt', 'com.upliftinc.android.txt', 'com.grainger.mobile.android.txt', 'com.hasbro.hasbropulse.txt', 'org.craigslist.CraigslistMobile.txt', 'com.samsung.ecomm.txt', 'com.roomstogo.dealstogo.txt', 'com.app.disposeit.txt', 'co.tapcart.app.id_K2pNP2pYor.txt', 'yqtrack.app.txt', 'com.ingka.ikea.app.txt', 'com.dealdash.txt', 'info.sanyaairjordanv.app.txt', 'com.whutsfree.freestuff.txt', 'com.RaceTrac.Common.txt', 'com.safeway.client.android.jewelosco.txt', 'com.lazada.android.txt', 'com.iherb.txt', 'com.nike.omega.txt', 'com.sezzle.sezzlemobile.txt', 'com.nike.snkrs.txt', 'com.zulily.android.txt', 'com.mercariapp.mercari.txt', 'com.backmarket.txt', 'com.harborfreight.app.txt', 'com.myklarnamobile.txt', 'com.follow.ElfCosmetics.txt', 'com.offerup.txt', 'com.wishabi.flipp.txt', 'com.route.app.txt', 'com.affirm.central.txt', 'com.romwe.txt', 'com.ae.ae.txt', 'com.zaful.txt', 'com.flipkart.android.txt', 'com.coupons.ciapp.txt', 'com.target.ui.txt', 'com.cupshe.cupshe.txt', 'com.brandingbrand.reactnative.and.bjs.txt', 'com.zappos.android.txt', 'com.thirdrock.fivemiles.txt', 'com.usablenet.mobile.walgreen.txt', 'com.ncp.ncpmobile.txt', 'com.dhgate.buyermob.txt', 'com.einnovation.temu.txt', 'com.alibaba.aliexpresshd.txt', 'com.quadpay.quadpay.txt', 'com.cargurus.mobileApp.txt', 'com.etsy.android.txt', 'com.costco.app.android.txt', 'com.bestbuy.android.txt', 'com.opensooq.OpenSooq.txt', 'com.lyst.lystapp.txt', 'com.joinhoney.honeyandroid.txt', 'com.treedollar.dollarshop.tree.txt', 'online.shoping.app.rpdapps2333.txt', 'com.haffprice.android.txt', 'com.kohls.mcommerce.opal.txt', 'com.childrensplace.tcpmobileapp.txt', 'com.nextmar.zeelool.android.txt', 'com.flamingo.shop.txt', 'com.x99only.mobileapp.txt', 'com.shopmium.txt', 'com.jcp.txt', 'com.flutter_use_redux.StyleweFashion.txt', 'com.globalegrow.app.dresslily.txt', 'com.rfi.sams.android.txt', 'org.shopgoodwill.app.txt', 'com.lightinthebox.android.txt', 'com.beatgridmedia.panelsync.mediarewards.txt', 'com.circlek.gmap.na.txt', 'com.flutter_use_redux.noracora.txt', 'com.poqstudio.app.platform.boohoo.txt', 'com.reverb.app.txt', 'com.doublefs.halara.txt', 'com.gamestop.powerup.txt', 'com.hottopic.android.txt', 'com.modesens.androidapp.txt', 'com.asos.app.txt', 'com.cat.digital.ecomm.txt', 'com.pogotech.pogocash.txt', 'dsgui.android.txt', 'com.fetchrewards.fetchrewards.hop.txt', 'com.ebates.txt', 'com.academy.android.txt', 'com.shipt.groceries.txt', 'com.biddl.biddl.txt', 'com.mercatustechnologies.staterbros.txt', 'com.wikibuy.prod.main.txt', 'com.thehomedepot.txt', 'com.puma.ecom.app.txt', 'com.poqstudio.app.platform.plt.txt']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "dir_path=\"shop/process\"\n",
    "openai.api_key = \"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\"\n",
    "cnt=0\n",
    "file_list=[]\n",
    "for file in os.listdir(dir_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        body = open(os.path.join(dir_path,file), 'r').read()\n",
    "        \n",
    "        try:\n",
    "            rsp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": \"I want you to act as a privacy policy checker.If the input contains privacy policy,output 1. If the input doesnot contain privacy policy,output 0. Only reply 0 or 1. Do not answer anything else. Do not write explanations on replies.\"},\n",
    "            {\"role\": \"user\", \"content\": body},\n",
    "            ]\n",
    "            )\n",
    "        \n",
    "            answer=rsp.get(\"choices\")[0][\"message\"][\"content\"]\n",
    "        except:\n",
    "            answer=\"0\"\n",
    "            time.sleep(10)\n",
    "        if answer==\"0\":\n",
    "            cnt+=1\n",
    "        elif answer==\"1\":\n",
    "            file_list.append(file)\n",
    "        else:\n",
    "            print(\"output error\")\n",
    "print(cnt)\n",
    "print(file_list)\n",
    "#write to json\n",
    "with open('shop.json', 'w+') as f:\n",
    "    json.dump(file_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    }
   ],
   "source": [
    "with open('shop.json', 'r') as f:\n",
    "    file_list = json.load(f)\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2940550/1388345406.py\", line 21, in <cell line: 18>\n",
      "    rsp = openai.ChatCompletion.create(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 288, in request\n",
      "    result = self.request_raw(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 596, in request_raw\n",
      "    result = _thread_context.session.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/requests/adapters.py\", line 486, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 790, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 536, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/urllib3/connection.py\", line 454, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/http/client.py\", line 1348, in getresponse\n",
      "    response.begin()\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/http/client.py\", line 316, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/http/client.py\", line 277, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "with open('music.json', 'r') as f:\n",
    "    file_list = json.load(f)\n",
    "dir_path=\"music/process\"\n",
    "output_path=\"music/privacy\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "openai.api_key = \"\"\n",
    "example1=open(\"example1.txt\", 'r').read()\n",
    "example2=open(\"example2.txt\", 'r').read()\n",
    "example3=open(\"example3.txt\", 'r').read()\n",
    "example4=open(\"example4.txt\", 'r').read()\n",
    "cnt=0\n",
    "for file in file_list:\n",
    "        body = open(os.path.join(dir_path,file), 'r').read()\n",
    "        try:\n",
    "            rsp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": \"a privacy label generator, reply with a privacy policy in json format such as:\\\"{}\\\",\\\"{}\\\",\\\"{}\\\",\\\"{}\\\"\" .format(example1,example2,example3,example4)},\n",
    "            {\"role\": \"user\", \"content\": body},\n",
    "            ]\n",
    "            )\n",
    "            answer=rsp.get(\"choices\")[0][\"message\"][\"content\"]\n",
    "            if answer[0]==\"{\" and answer[-1]==\"}\":\n",
    "                output_file=open(os.path.join(output_path,file), 'w+')\n",
    "                output_file.write(answer)\n",
    "                output_file.close()\n",
    "                cnt+=1\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "        time.sleep(3)\n",
    "print(cnt)\n",
    "        #save rsp to result.txt\n",
    "        \n",
    "        # print(rsp.get(\"choices\")[0][\"message\"][\"content\"])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3370903/2573775350.py\", line 21, in <cell line: 12>\n",
      "    rsp = openai.ChatCompletion.create(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7cc801b21afd2f119d74cfe45057f5d9 in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3370903/2573775350.py\", line 21, in <cell line: 12>\n",
      "    rsp = openai.ChatCompletion.create(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID ea4f07aff3f275c8ffa608deb2c45175 in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt error\n",
      "error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3370903/2573775350.py\", line 21, in <cell line: 12>\n",
      "    rsp = openai.ChatCompletion.create(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 8c71e2fc3223afd11620444ee003b3a4 in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt error\n",
      "error\n",
      "correct:83 incorrect:20 accuracy:80.58252427184466\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "dir_path=\"music/privacy\"\n",
    "process_dir_path=\"music/process\"\n",
    "origin_path=\"music\"\n",
    "correct=0\n",
    "incorrect=0\n",
    "openai.api_key = \"\"\n",
    "for file in os.listdir(dir_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        body = open(os.path.join(dir_path,file), 'r').read()\n",
    "        policy=open(os.path.join(process_dir_path,file), 'r').read()\n",
    "        #replace .txt as .json\n",
    "        file=file.replace(\".txt\",\".json\")\n",
    "        gt=open(os.path.join(origin_path,file), 'r').read()\n",
    "        \n",
    "        try:\n",
    "            rsp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Compare original privacy label and generated privacy label. If the generated privacy label is better, output 1. Otherwise, output 0. Only reply 0 or 1. Do not answer anything else. Do not write explanations on replies.\"},\n",
    "            {\"role\": \"user\", \"content\":\"privacy label:{}, generated privacy label:{}, privacy policy:{}\".format(gt,body,policy) },\n",
    "            ]\n",
    "            )\n",
    "        \n",
    "            answer=rsp.get(\"choices\")[0][\"message\"][\"content\"]\n",
    "            \n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            answer=\"sb\"\n",
    "            print(\"gpt error\")\n",
    "        \n",
    "            time.sleep(30)\n",
    "        if answer==\"0\":\n",
    "            incorrect+=1\n",
    "        elif answer==\"1\":\n",
    "            correct+=1\n",
    "        else:\n",
    "            print(\"error\")\n",
    "        time.sleep(3)\n",
    "            \n",
    "print(\"correct:{} incorrect:{} accuracy:{}\".format(correct,incorrect,100.0*correct/(correct+incorrect)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct:118 incorrect:13 accuracy:90.07633587786259 tool\n",
    "#correct:73 incorrect:22 accuracy:76.84210526315789 shop\n",
    "#correct:101 incorrect:10 accuracy:90.990990990991 social\n",
    "#correct:83 incorrect:20 accuracy:80.58252427184466 music\n",
    "# wrrite a markdown format table\n",
    "# |Field|Better|Worse|Ratio|\n",
    "# |---|---|---|---|\n",
    "# |Tool|118|13|90.07633587786259|\n",
    "# |Shopping|73|22|76.84210526315789|\n",
    "# |Social|101|10|90.990990990991|\n",
    "# |Music|83|20|80.58252427184466|\n",
    "# |Total|375|65|85.24590163934425|\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Field|Better|Worse|Ratio|\n",
    "|---|---|---|---|\n",
    "|Tool|118|13|90.07633587786259|\n",
    "|Shopping|73|22|76.84210526315789|\n",
    "|Social|101|10|90.990990990991|\n",
    "|Music|83|20|80.58252427184466|\n",
    "|Total|375|65|85.24590163934425|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_338365/1060838017.py\", line 24, in <cell line: 15>\n",
      "    rsp = openai.ChatCompletion.create(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 6cb4d814b482e43a9bbefaa8b7f34422 in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt error\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_338365/1060838017.py\", line 24, in <cell line: 15>\n",
      "    rsp = openai.ChatCompletion.create(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 298, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 700, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"/home/jielin/anaconda3/envs/bert/lib/python3.8/site-packages/openai/api_requestor.py\", line 763, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d239605ae5206ccb498335353a2bcc9b in your message.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt error\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "dir_path=\"music/privacy\"\n",
    "process_dir_path=\"music/process\"\n",
    "output_file_path=\"music/reason\"\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "origin_path=\"music\"\n",
    "correct=0\n",
    "incorrect=0\n",
    "openai.api_key = \"\"\n",
    "for file in os.listdir(dir_path):\n",
    "    if file.endswith(\".txt\"):\n",
    "        body = open(os.path.join(dir_path,file), 'r').read()\n",
    "        policy=open(os.path.join(process_dir_path,file), 'r').read()\n",
    "        #replace .txt as .json\n",
    "        file=file.replace(\".txt\",\".json\")\n",
    "        gt=open(os.path.join(origin_path,file), 'r').read()\n",
    "        \n",
    "        try:\n",
    "            rsp = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Compare original privacy label and generated privacy label. Explain which one is better based on CCPA/GDPR.\"},\n",
    "            {\"role\": \"user\", \"content\":\"original privacy label:{}, generated privacy label:{}, privacy policy:{}\".format(gt,body,policy) },\n",
    "            ]\n",
    "            )\n",
    "        \n",
    "            answer=rsp.get(\"choices\")[0][\"message\"][\"content\"]\n",
    "            \n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            answer=\"sb\"\n",
    "            print(\"gpt error\")\n",
    "        \n",
    "            time.sleep(30)\n",
    "        output_file=open(os.path.join(output_file_path,file), 'w+')\n",
    "        output_file.write(answer)\n",
    "        output_file.close()\n",
    "        time.sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarize",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
